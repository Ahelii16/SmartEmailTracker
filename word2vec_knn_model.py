# -*- coding: utf-8 -*-
"""Word2vec_KNN_model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zzafuw-ssqzaKFV08-qA1z1xywAXCu3f

# Few-Shot Learning Email Classification with Pre-Trained Word2Vec Embeddings
"""

import pandas as pd
import numpy as np
from random import seed
from random import sample

seed(42)
np.random.seed(42)

from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import re
import gensim.downloader as api
from gensim.models.keyedvectors import Word2VecKeyedVectors

from sklearn.decomposition import PCA
from sklearn.metrics import accuracy_score
from scipy import spatial

from nltk.corpus import stopwords

model = api.load('word2vec-google-news-300')

from google.colab import files
uploaded = files.upload()

df = pd.read_csv("./emaildataset.csv", usecols = ['Subject','Body', 'Class'])
df.head()

def get_only_chars(line):

    clean_line = ""

    line = line.replace("’", "")
    line = line.replace("'", "")
    line = line.replace("-", " ") #replace hyphens with spaces
    line = line.replace("\t", " ")
    line = line.replace("\n", " ")
    line = line.lower()

    for char in line:
        if char in 'qwertyuiopasdfghjklzxcvbnm ':
            clean_line += char
        else:
            clean_line += ' '

    clean_line = re.sub(' +',' ',clean_line) #delete extra spaces
    if clean_line[0] == ' ':
        clean_line = clean_line[1:]
    return clean_line

# subj, body to text

for i in range(df.shape[0]):
    # merge subject and body strings
    df['Text'] = (df['Subject'] + " " + df['Body'])

df['Text'] = df['Text'].apply(lambda x: get_only_chars(x))



df = df.drop_duplicates('Text')



# set the by default to :

num_classes = 2 # the number of classes we consider (since the dataset has many classes)
sample_size = 3 # the number of labeled sampled we’ll require from the user



from sklearn.preprocessing import LabelEncoder 

le = LabelEncoder()
# classes = df['Class'].values

# classes = classes.reshape(-1, 1)
# Y = le.fit_transform(classes)
df['Class'] = le.fit_transform(df['Class'])

df.head()
# df.shape

df['Class'] = df['Class'].apply(lambda x : x + 1)



# Generate samples that contains K samples of each class

def gen_sample(sample_size, num_classes):

    df_1 = df[(df["Class"] < num_classes+1)].reset_index().drop(["index"], axis=1).reset_index().drop(["index"], axis=1)
    train = df_1[df_1["Class"] == np.unique(df_1['Class'])[0]].sample(sample_size)

    train_index = train.index.tolist()

    for i in range(1,num_classes):
        train_2 = df_1[df_1["Class"] == np.unique(df_1['Class'])[i]].sample(sample_size)
        train = pd.concat([train, train_2], axis=0)
        train_index.extend(train_2.index.tolist())

    test = df_1[~df_1.index.isin(train_index)]
    # return test
    return train, test

# test = gen_sample(sample_size, num_classes)
# test.head(30)



# Apply that to the dataframe :

train, test = gen_sample(sample_size, num_classes)

X_train = train['Text']
y_train = train['Class'].values
X_test = test['Text']
y_test = test['Class'].values

# train.head(20)
print(train.values[2])

# test.head(20)

# print(X_train)



print(X_train.shape)



# Text processing (split, find token id, get embedidng)
def transform_sentence(text, model):

    """
    Mean embedding vector
    """

    def preprocess_text(raw_text, model=model):

        """ 
        Excluding unknown words and get corresponding token
        """

        raw_text = raw_text.split()

        return list(filter(lambda x: x in model.vocab, raw_text))

    tokens = preprocess_text(text)

    if not tokens:
        return np.zeros(model.vector_size)

    text_vector = np.mean(model[tokens], axis=0)

    return np.array(text_vector)



# Apply this to both the train and the test :

X_train_mean = X_train.apply(lambda x : transform_sentence(x, model))
X_test_mean = X_test.apply(lambda x : transform_sentence(x, model))

X_train_mean = pd.DataFrame(X_train_mean)['Text'].apply(pd.Series)
X_test_mean = pd.DataFrame(X_test_mean)['Text'].apply(pd.Series)



# Use cosine similarity to find closest class

def classify_txt(txt, mean_embedding):

    best_dist = 1
    best_label = -1

    for cl in range(num_classes):

        dist = spatial.distance.cosine(transform_sentence(txt, model), mean_embedding[cl])

        if dist < best_dist :
            best_dist = dist
            best_label = cl + 1 # not cl?

    return best_label



# Process text and predict on the test set

def return_score(sample_size, num_classes):

    train, test = gen_sample(sample_size, num_classes)

    X_train = train['Text']
    y_train = train['Class'].values
    X_test = test['Text']
    y_test = test['Class'].values

    X_train_mean = X_train.apply(lambda x : transform_sentence(x, model))
    X_test_mean = X_test.apply(lambda x : transform_sentence(x, model))

    X_train_mean = pd.DataFrame(X_train_mean)['Text'].apply(pd.Series)
    X_test_mean = pd.DataFrame(X_test_mean)['Text'].apply(pd.Series)

    mean_embedding = {}
    for cl in range(num_classes):
        mean_embedding[cl] = np.mean((X_train_mean[y_train == cl + 1]), axis=0)

    y_pred = [classify_txt(t, mean_embedding) for t in test['Text'].values]

    return accuracy_score(y_pred, y_test)



# Now, we will iterate on the number of classes (between 2 and 7) and the number of samples (between 1 and 30). 
# We will consider that labeling more than 30 training examples per class is too long.

all_accuracy = {2:[],3:[],4:[],5:[],6:[],7:[]}

for num_samples in range(1,30):
    for num_cl in range(2, 7):
        all_accuracy[num_cl].append(return_score(num_samples,num_cl))



# plotting accuracy for each no. of class, depending on the number of train examples :

plt.figure(figsize=(12,8))
plt.plot(all_accuracy[2], label="2 classes")
plt.plot(all_accuracy[3], label="3 classes")
plt.plot(all_accuracy[4], label="4 classes")
plt.plot(all_accuracy[5], label="5 classes")
plt.plot(all_accuracy[6], label="6 classes")
plt.axvline(7, c='black', alpha=0.5)
plt.title("Accuracy depending on the number of samples and classes")
plt.legend()
plt.show()



"""## Pre-trained Word2Vec and ML algorithms"""

from sklearn.neighbors import KNeighborsClassifier
from sklearn.neighbors import RadiusNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import LinearSVC
from sklearn import svm, tree
from sklearn.naive_bayes import MultinomialNB
import xgboost
from sklearn.model_selection import cross_val_score

def return_score_knn(sample_size, num_classes):

    train, test = gen_sample(sample_size, num_classes)

    X_train = train['Text']
    y_train = train['Class'].values
    X_test = test['Text']
    y_test = test['Class'].values

    X_train_mean = X_train.apply(lambda x : transform_sentence(x, model))
    X_test_mean = X_test.apply(lambda x : transform_sentence(x, model))

    X_train_mean = pd.DataFrame(X_train_mean)['Text'].apply(pd.Series)
    X_test_mean = pd.DataFrame(X_test_mean)['Text'].apply(pd.Series)

    # K nearest neighbours algorithm (lazy learner)
    clf = KNeighborsClassifier(n_neighbors=sample_size, p=2) # euclidean distance
    clf.fit(X_train_mean, y_train)

    y_pred = clf.predict(X_test_mean)

    return accuracy_score(y_pred, y_test)

def return_score_radius(sample_size, num_classes):

    train, test = gen_sample(sample_size, num_classes)

    X_train = train['Text']
    y_train = train['Class'].values
    X_test = test['Text']
    y_test = test['Class'].values

    X_train_mean = X_train.apply(lambda x : transform_sentence(x, model))
    X_test_mean = X_test.apply(lambda x : transform_sentence(x, model))

    X_train_mean = pd.DataFrame(X_train_mean)['Text'].apply(pd.Series)
    X_test_mean = pd.DataFrame(X_test_mean)['Text'].apply(pd.Series)

    # Radius neighbours
    clf2 = RadiusNeighborsClassifier(radius=1.0, outlier_label='most_frequent')
    clf2.fit(X_train_mean, y_train)

    y_pred = clf2.predict(X_test_mean)

    return accuracy_score(y_pred, y_test)

def return_score_nb(sample_size, num_classes):

    train, test = gen_sample(sample_size, num_classes)

    X_train = train['Text']
    y_train = train['Class'].values
    X_test = test['Text']
    y_test = test['Class'].values

    X_train_mean = X_train.apply(lambda x : transform_sentence(x, model))
    X_test_mean = X_test.apply(lambda x : transform_sentence(x, model))

    X_train_mean = pd.DataFrame(X_train_mean)['Text'].apply(pd.Series)
    X_test_mean = pd.DataFrame(X_test_mean)['Text'].apply(pd.Series)

    # Naive Bayes (eager learner)
    clf = MultinomialNB()
    clf.fit(X_train_mean, y_train)

    y_pred = clf.predict(X_test_mean)

    return accuracy_score(y_pred, y_test)

def return_score_svc(sample_size, num_classes):

    train, test = gen_sample(sample_size, num_classes)

    X_train = train['Text']
    y_train = train['Class'].values
    X_test = test['Text']
    y_test = test['Class'].values

    X_train_mean = X_train.apply(lambda x : transform_sentence(x, model))
    X_test_mean = X_test.apply(lambda x : transform_sentence(x, model))

    X_train_mean = pd.DataFrame(X_train_mean)['Text'].apply(pd.Series)
    X_test_mean = pd.DataFrame(X_test_mean)['Text'].apply(pd.Series)

    # SVC
    clf = svm.SVC()
    clf.fit(X_train_mean, y_train)

    y_pred = clf.predict(X_test_mean)

    return accuracy_score(y_pred, y_test)

def return_score_rf(sample_size, num_classes):

    train, test = gen_sample(sample_size, num_classes)

    X_train = train['Text']
    y_train = train['Class'].values
    X_test = test['Text']
    y_test = test['Class'].values

    X_train_mean = X_train.apply(lambda x : transform_sentence(x, model))
    X_test_mean = X_test.apply(lambda x : transform_sentence(x, model))

    X_train_mean = pd.DataFrame(X_train_mean)['Text'].apply(pd.Series)
    X_test_mean = pd.DataFrame(X_test_mean)['Text'].apply(pd.Series)

    # Random forest classifier
    clf = RandomForestClassifier(n_estimators=200, max_depth=3, random_state=0)
    clf.fit(X_train_mean, y_train)

    y_pred = clf.predict(X_test_mean)

    return accuracy_score(y_pred, y_test)

def return_score_linsvc(sample_size, num_classes):

    train, test = gen_sample(sample_size, num_classes)

    X_train = train['Text']
    y_train = train['Class'].values
    X_test = test['Text']
    y_test = test['Class'].values

    X_train_mean = X_train.apply(lambda x : transform_sentence(x, model))
    X_test_mean = X_test.apply(lambda x : transform_sentence(x, model))

    X_train_mean = pd.DataFrame(X_train_mean)['Text'].apply(pd.Series)
    X_test_mean = pd.DataFrame(X_test_mean)['Text'].apply(pd.Series)

    # Linear SVC
    clf = LinearSVC()
    clf.fit(X_train_mean, y_train)

    y_pred = clf.predict(X_test_mean)

    return accuracy_score(y_pred, y_test)

def return_score_xgb(sample_size, num_classes):

    train, test = gen_sample(sample_size, num_classes)

    X_train = train['Text']
    y_train = train['Class'].values
    X_test = test['Text']
    y_test = test['Class'].values

    X_train_mean = X_train.apply(lambda x : transform_sentence(x, model))
    X_test_mean = X_test.apply(lambda x : transform_sentence(x, model))

    X_train_mean = pd.DataFrame(X_train_mean)['Text'].apply(pd.Series)
    X_test_mean = pd.DataFrame(X_test_mean)['Text'].apply(pd.Series)

    # XG Boost
    clf = xgboost.XGBClassifier()
    clf.fit(X_train_mean, y_train)

    y_pred = clf.predict(X_test_mean)

    return accuracy_score(y_pred, y_test)

def return_score_dt(sample_size, num_classes):

    train, test = gen_sample(sample_size, num_classes)

    X_train = train['Text']
    y_train = train['Class'].values
    X_test = test['Text']
    y_test = test['Class'].values

    X_train_mean = X_train.apply(lambda x : transform_sentence(x, model))
    X_test_mean = X_test.apply(lambda x : transform_sentence(x, model))

    X_train_mean = pd.DataFrame(X_train_mean)['Text'].apply(pd.Series)
    X_test_mean = pd.DataFrame(X_test_mean)['Text'].apply(pd.Series)

    # Decision tree
    clf = tree.DecisionTreeClassifier()
    clf.fit(X_train_mean, y_train)

    y_pred = clf.predict(X_test_mean)

    return accuracy_score(y_pred, y_test)



"""## Comparison of accuracies"""

all_accuracy_knn = {2:[],3:[],4:[],5:[],6:[],7:[]}

for num_samples in range(1,30):

    for num_cl in range(2, 7):

        all_accuracy_knn[num_cl].append(return_score_knn(num_samples,num_cl))

all_accuracy_radius = {2:[],3:[],4:[],5:[],6:[],7:[]}

for num_samples in range(1,30):

    for num_cl in range(2, 7):

        all_accuracy_radius[num_cl].append(return_score_radius(num_samples,num_cl))

# all_accuracy_nb = {2:[],3:[],4:[],5:[],6:[],7:[]}

# for num_samples in range(1,30):

#     for num_cl in range(2, 7):

#         all_accuracy_nb[num_cl].append(return_score_nb(num_samples,num_cl))

all_accuracy_svc = {2:[],3:[],4:[],5:[],6:[],7:[]}

for num_samples in range(1,30):

    for num_cl in range(2, 7):

        all_accuracy_svc[num_cl].append(return_score_svc(num_samples,num_cl))

all_accuracy_rf = {2:[],3:[],4:[],5:[],6:[],7:[]}

for num_samples in range(1,30):

    for num_cl in range(2, 7):

        all_accuracy_rf[num_cl].append(return_score_rf(num_samples,num_cl))

all_accuracy_linsvc = {2:[],3:[],4:[],5:[],6:[],7:[]}

for num_samples in range(1,30):

    for num_cl in range(2, 7):

        all_accuracy_linsvc[num_cl].append(return_score_linsvc(num_samples,num_cl))

all_accuracy_xgb = {2:[],3:[],4:[],5:[],6:[],7:[]}

for num_samples in range(1,30):

    for num_cl in range(2, 7):

        all_accuracy_xgb[num_cl].append(return_score_xgb(num_samples,num_cl))

all_accuracy_dt = {2:[],3:[],4:[],5:[],6:[],7:[]}

for num_samples in range(1,30):

    for num_cl in range(2, 7):

        all_accuracy_dt[num_cl].append(return_score_dt(num_samples,num_cl))



plt.figure(figsize=(12,8))
plt.plot(all_accuracy_knn[5], label="K nearest neighbours")
plt.plot(all_accuracy_radius[5], label="Radius neighbours")
# plt.plot(all_accuracy_nb[5], label="Multinomial naive bayes")
plt.plot(all_accuracy_svc[5], label="Support vector machine")
plt.plot(all_accuracy_rf[5], label="Random forest")
plt.plot(all_accuracy_linsvc[5], label="Linear SVC")
plt.plot(all_accuracy_xgb[5], label="XG Boost")
plt.plot(all_accuracy_dt[5], label="Decision tree")

plt.title("Accuracy depending on the number of classes and chosen algorithm")
plt.legend()
plt.show()

"""## Summary"""

# df_results = pd.DataFrame({
#     'Nb Classes':[2,3,4,5], 
#     'min K-NN':[min(all_accuracy_knn[2]), 
#         min(all_accuracy_knn[3]), 
#         min(all_accuracy_knn[4]), 
#         min(all_accuracy_knn[5])],
#     'min Cosine':[min(all_accuracy[2]), 
#         min(all_accuracy[3]), 
#         min(all_accuracy[4]), 
#         min(all_accuracy[5])],
#     'mean K-NN':[np.mean(all_accuracy_knn[2]), 
#         np.mean(all_accuracy_knn[3]), 
#         np.mean(all_accuracy_knn[4]), 
#         np.mean(all_accuracy_knn[5])],
#     'mean Cosine':[np.mean(all_accuracy[2]), 
#         np.mean(all_accuracy[3]), 
#         np.mean(all_accuracy[4]), 
#         np.mean(all_accuracy[5])],
#     'max K-NN':[max(all_accuracy_knn[2]), 
#         max(all_accuracy_knn[3]), 
#         max(all_accuracy_knn[4]), 
#         max(all_accuracy_knn[5])],
#     'max Cosine':[max(all_accuracy[2]), 
#         max(all_accuracy[3]), 
#         max(all_accuracy[4]), 
#         max(all_accuracy[5])]
#     })

df_results = pd.DataFrame({
    
    'Nb Classes':[2, 3, 4, 5, 6], 

    'mean K-NN':[np.mean(all_accuracy_knn[2]), 
        np.mean(all_accuracy_knn[3]), 
        np.mean(all_accuracy_knn[4]), 
        np.mean(all_accuracy_knn[5]),
        np.mean(all_accuracy_knn[6])],
    'max K-NN':[max(all_accuracy_knn[2]), 
        max(all_accuracy_knn[3]), 
        max(all_accuracy_knn[4]), 
        max(all_accuracy_knn[5]),
        max(all_accuracy_knn[6])],

    'mean Cosine':[np.mean(all_accuracy[2]), 
        np.mean(all_accuracy[3]), 
        np.mean(all_accuracy[4]), 
        np.mean(all_accuracy[5]),
        np.mean(all_accuracy[6])], 
    'max Cosine':[max(all_accuracy[2]), 
        max(all_accuracy[3]), 
        max(all_accuracy[4]), 
        max(all_accuracy[5]),
        max(all_accuracy[6])],

    'mean R-N':[np.mean(all_accuracy_radius[2]), 
        np.mean(all_accuracy_radius[3]), 
        np.mean(all_accuracy_radius[4]), 
        np.mean(all_accuracy_radius[5]),
        np.mean(all_accuracy_radius[6])],
    'max R-N':[max(all_accuracy_radius[2]), 
        max(all_accuracy_radius[3]), 
        max(all_accuracy_radius[4]), 
        max(all_accuracy_radius[5]),
        max(all_accuracy_radius[6])],

    # 'mean Naive Bayes':[np.mean(all_accuracy_nb[2]), 
    #     np.mean(all_accuracy_nb[3]), 
    #     np.mean(all_accuracy_nb[4]), 
    #     np.mean(all_accuracy_nb[5]),
    #     np.mean(all_accuracy_nb[6])],
    # 'max Naive Bayes':[max(all_accuracy_nb[2]), 
    #     max(all_accuracy_nb[3]), 
    #     max(all_accuracy_nb[4]), 
    #     max(all_accuracy_nb[5]),
    #     max(all_accuracy_nb[6])],

    'mean SVM':[np.mean(all_accuracy_svc[2]), 
        np.mean(all_accuracy_svc[3]), 
        np.mean(all_accuracy_svc[4]), 
        np.mean(all_accuracy_svc[5]),
        np.mean(all_accuracy_svc[6])],
    'max SVM':[max(all_accuracy_svc[2]), 
        max(all_accuracy_svc[3]), 
        max(all_accuracy_svc[4]), 
        max(all_accuracy_svc[5]),
        max(all_accuracy_svc[6])],

    'mean Linear SVC':[np.mean(all_accuracy_linsvc[2]), 
        np.mean(all_accuracy_linsvc[3]), 
        np.mean(all_accuracy_linsvc[4]), 
        np.mean(all_accuracy_linsvc[5]),
        np.mean(all_accuracy_linsvc[6])],
    'max Linear SVC':[max(all_accuracy_linsvc[2]), 
        max(all_accuracy_linsvc[3]), 
        max(all_accuracy_linsvc[4]), 
        max(all_accuracy_linsvc[5]),
        max(all_accuracy_linsvc[6])],

    'mean XG Boost':[np.mean(all_accuracy_xgb[2]), 
        np.mean(all_accuracy_xgb[3]), 
        np.mean(all_accuracy_xgb[4]), 
        np.mean(all_accuracy_xgb[5]),
        np.mean(all_accuracy_xgb[6])],
    'max XG Boost':[max(all_accuracy_xgb[2]), 
        max(all_accuracy_xgb[3]), 
        max(all_accuracy_xgb[4]), 
        max(all_accuracy_xgb[5]),
        max(all_accuracy_xgb[6])],

    'mean Decision Tree':[np.mean(all_accuracy_dt[2]), 
        np.mean(all_accuracy_dt[3]), 
        np.mean(all_accuracy_dt[4]), 
        np.mean(all_accuracy_dt[5]),
        np.mean(all_accuracy_dt[6])],
    'max Decision Tree':[max(all_accuracy_dt[2]), 
        max(all_accuracy_dt[3]), 
        max(all_accuracy_dt[4]), 
        max(all_accuracy_dt[5]),
        max(all_accuracy_dt[6])],

    'mean Random Forest':[np.mean(all_accuracy_rf[2]), 
        np.mean(all_accuracy_rf[3]), 
        np.mean(all_accuracy_rf[4]), 
        np.mean(all_accuracy_rf[5]),
        np.mean(all_accuracy_rf[6])],
    'max Random Forest':[max(all_accuracy_rf[2]), 
        max(all_accuracy_rf[3]), 
        max(all_accuracy_rf[4]), 
        max(all_accuracy_rf[5]),
        max(all_accuracy_rf[6])]
    })

df_results

